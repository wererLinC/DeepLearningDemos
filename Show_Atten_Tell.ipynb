{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import gfile\n",
    "import tensorflow.compat.v1.logging as logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from tensorflow import keras\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 设置内存自增长\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# # 打印物理GPU有几个，就是电脑实际装的个数\n",
    "# print(len(gpus))\n",
    "# for gpu in gpus:\n",
    "#     # 设置 GPU 所占用内存自动增长\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_description_file = \"../dataset/results_20130124.token\"\n",
    "input_img_feature_dir = \"../dataset/feature_extraction_inception_v3\"\n",
    "input_vocab_file = \"../dataset/vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "    <img src='../image/词频.png', width= 600, height = 200>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        '''\n",
    "            filename:词表文件\n",
    "            word_num_threshold:词的个数少于几个就省略掉？\n",
    "        '''\n",
    "        # 用于解码， id到词的映射\n",
    "        self._id_to_word = {}\n",
    "        # 用于编码， 词到id的映射\n",
    "        self._word_to_id = {}\n",
    "        # 未知字符\n",
    "        self._unk = -1\n",
    "        # 开始字符\n",
    "        self._start = -1\n",
    "        # 结束字符\n",
    "        self._eos = -1\n",
    "        # 前面我们统计过的词频，太少我们就不要了\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            # 词出现的次数\n",
    "            occurence = int(occurence)\n",
    "            # 如果词出现的频次太少了，我们就跳过\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            # 从顺序开始，第一次出现的词，对应的id为0， 以此类推\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '<s>':\n",
    "                self._start = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception(\"重复添加！！！\")\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def start(self):\n",
    "        return self._start\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    # 单个词到id的转换\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "    # 单个id到词的转换\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')\n",
    "    # 整个词表的大小\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    # 编码，把句子转换成id  用于训练模型前的编码\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "    # 解码，把id数组转化成句子， 预测时，模型输出的是id， 用于解码\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个大家应该是特别熟悉了，我们第一次就讲的这个\n",
    "def parse_token_file(token_file):\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把描述的句子转换成id\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    img_name_to_token_ids = {}\n",
    "    # 拿到每张图片多个描述\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_token_ids.setdefault(img_name, [])\n",
    "        descriptions = img_name_to_tokens[img_name]\n",
    "        for description in descriptions:\n",
    "            # 进行编码\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_token_ids[img_name].append(token_ids)\n",
    "    return img_name_to_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 9223\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "[[4, 10, 5, 133, 9, 3533, 7, 2, 49, 338, 147, 140, 2, 245, 94, 8, 381, 37, 3],\n",
      " [4, 21, 180, 12, 2, 27, 285, 8, 121, 129, 298, 7, 2, 94, 147, 3],\n",
      " [4, 10, 5, 2, 27, 22, 8, 121, 129, 9, 341, 7, 2, 94, 147, 3],\n",
      " [4, 64, 9, 341, 7, 2, 147, 13, 71, 16, 519, 3],\n",
      " [4, 10, 341, 7, 2, 147, 5, 2, 113, 172, 3]]\n"
     ]
    }
   ],
   "source": [
    "# 词的次数出现少于4次的我们用 <UNK> 进行表示\n",
    "vocab = Vocab(input_vocab_file, 4)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "\n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_token_ids = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:10])\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_ids))\n",
    "pprint.pprint(list(img_name_to_token_ids.keys())[0:10])\n",
    "pprint.pprint(img_name_to_token_ids['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "    <img src='../image/编码.png', width= 600, height = 200>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/feature_extraction_inception_v3\\\\image_features-0.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-1.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-10.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-100.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-101.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-102.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-103.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-104.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-105.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-106.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-107.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-108.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-109.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-11.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-110.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-111.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-112.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-113.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-114.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-115.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-116.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-117.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-118.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-119.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-12.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-120.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-121.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-122.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-123.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-124.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-125.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-126.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-127.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-128.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-129.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-13.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-130.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-131.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-132.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-133.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-134.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-135.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-136.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-137.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-138.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-139.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-14.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-140.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-141.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-142.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-143.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-144.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-145.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-146.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-147.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-148.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-149.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-15.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-150.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-151.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-152.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-153.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-154.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-155.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-156.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-157.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-158.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-159.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-16.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-160.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-161.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-162.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-163.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-164.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-165.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-166.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-167.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-168.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-169.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-17.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-170.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-171.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-172.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-173.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-174.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-175.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-176.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-177.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-178.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-179.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-18.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-180.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-181.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-182.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-183.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-184.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-185.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-186.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-187.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-188.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-189.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-19.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-190.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-191.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-192.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-193.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-194.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-195.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-196.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-197.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-198.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-199.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-2.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-20.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-200.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-201.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-202.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-203.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-204.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-205.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-206.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-207.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-208.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-209.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-21.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-210.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-211.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-212.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-213.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-214.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-215.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-216.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-217.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-218.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-219.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-22.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-220.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-221.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-222.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-223.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-224.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-225.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-226.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-227.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-228.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-229.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-23.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-230.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-231.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-232.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-233.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-234.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-235.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-236.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-237.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-238.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-239.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-24.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-240.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-241.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-242.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-243.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-244.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-245.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-246.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-247.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-248.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-249.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-25.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-250.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-251.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-252.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-253.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-254.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-255.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-256.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-257.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-258.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-259.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-26.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-260.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-261.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-262.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-263.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-264.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-265.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-266.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-267.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-268.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-269.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-27.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-270.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-271.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-272.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-273.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-274.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-275.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-276.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-277.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-278.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-279.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-28.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-280.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-281.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-282.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-283.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-284.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-285.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-286.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-287.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-288.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-289.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-29.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-290.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-291.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-292.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-293.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-294.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-295.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-296.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-297.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-298.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-299.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-3.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-30.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-300.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-301.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-302.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-303.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-304.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-305.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-306.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-307.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-308.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-309.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-31.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-310.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-311.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-312.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-313.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-314.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-315.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-316.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-317.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-32.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-33.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-34.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-35.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-36.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-37.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-38.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-39.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-4.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-40.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-41.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-42.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-43.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-44.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-45.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-46.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-47.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-48.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-49.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-5.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-50.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-51.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-52.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-53.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-54.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-55.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-56.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-57.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-58.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-59.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-6.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-60.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-61.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-62.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-63.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-64.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-65.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-66.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-67.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-68.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-69.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-7.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-70.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-71.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-72.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-73.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-74.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-75.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-76.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-77.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-78.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-79.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-8.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-80.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-81.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-82.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-83.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-84.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-85.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-86.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-87.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-88.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-89.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-9.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-90.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-91.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-92.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-93.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-94.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-95.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-96.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-97.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-98.pickle',\n",
      " '../dataset/feature_extraction_inception_v3\\\\image_features-99.pickle']\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 img_name_to_token_ids,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        '''\n",
    "            img_name_to_token_ids:图片名字 -> 描述 \n",
    "            img_feature_dir：特征的文件夹\n",
    "            num_timesteps：固定句子的长度， 因为可能有些句子虽然很长，但是出现的次数很少\n",
    "            vocab： 我们词表的类\n",
    "            deterministic：是否进行 shuffle， 默认是 shuffle\n",
    "        '''\n",
    "        self._vocab = vocab\n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.listdir(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "\n",
    "        self._img_name_to_token_ids = img_name_to_token_ids\n",
    "        # 固定一个句子的长度\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        # 是否进行shuffle\n",
    "        self._deterministic = deterministic\n",
    "        \n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        self._load_img_feature_pickle()\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "    # 载入提取好的特征图片\n",
    "    def _load_img_feature_pickle(self):\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                # 之前使用pickle存储的，现在我们使用pickle拿出来\n",
    "                filenames, features = pickle.load(f, encoding='iso-8859-1')\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        # [feature1, feature2, feature3] ===> [feature1, \n",
    "        #                                      feature2, \n",
    "        #                                      feature3]\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(\"img_feature_data shape is: \", self._img_feature_data.shape)\n",
    "        print(\"img_feature_filenames shape is :\", self._img_feature_filenames.shape)\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "    # 有多少张图片\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "    # 提取到特征的大小\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "    # 进行下标打乱\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "    # 批次图片的描述转成id\n",
    "    def _img_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._img_name_to_token_ids[filename]\n",
    "            # 因为一张图片是对应多个描述，这边我们选用第一个描述\n",
    "            chosen_token_ids = token_ids_set[0]\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "            # 为什么会有weight， 因为描述的长度小于我们固定的长度，那么我们不做长句的惩罚.\n",
    "            # 假设我们选取的句子长度为5\n",
    "            # i love you . .  模型：i love you very much.   weight:[1, 1, 1, 0, 0]\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            # 如果句子长度大于我们固定的长度\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            chosen_token_ids.insert(0, 1)\n",
    "            chosen_token_ids.append(3) # 是我们结束字符的id .\n",
    "            # 对于这两个地方,我们就不计算权重\n",
    "            weight.insert(0, 0)\n",
    "            weight.append(0)\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "    # 下一个批次\n",
    "    def next(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        # 如果已经取到文件末尾了，是否进行shuffle，还有就是把其实的index置为0\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        batch_img_names = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_img_names)\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_img_names\n",
    "\n",
    "\n",
    "caption_data = ImageCaptionData(img_name_to_token_ids, input_img_feature_dir, 10, vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "    <img src='../image/框架图1.png', width= 600, height = 200>\n",
    "    <img src='../image/框架图2.png', width= 600, height = 200>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们的图像经过特征提取网络出来之后是 7*7*512\n",
    "# show attention and tell 是 14*14*512， 但是原理一样就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output size is : (32, 49, 512)\n"
     ]
    }
   ],
   "source": [
    "# 只是得到图片特征的 encoding_output\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense = keras.layers.Dense(512)\n",
    "        # return_sequences: 每一步是否有输出\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        # 对特征进行融合，再 reshape 成 (batch_size, 7*7, 512)\n",
    "        # 那么怎么进行融合呢，其实很简单\n",
    "        x = tf.reshape(x, [-1, 512])\n",
    "        # x shape == (-1, 512)\n",
    "        x = self.dense(x)\n",
    "        # x shape == (batch_size, 7*7, 512)\n",
    "        x = tf.reshape(x, [-1, 7*7, 512])\n",
    "        return x\n",
    "\n",
    "\n",
    "# 我们的 encoder 主要是对图片进行attention\n",
    "inputs = tf.random.normal([32, 7, 7, 512])\n",
    "encoder = Encoder()\n",
    "encoder_output = encoder(inputs)\n",
    "print(\"encoder_output size is :\", encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_result shape is : (32, 512)\n",
      "attention_weights shape is : (32, 49, 1)\n"
     ]
    }
   ],
   "source": [
    "# 这个是对特征图的 attention\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(512)\n",
    "        self.W2 = tf.keras.layers.Dense(512)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, hidden_state, encoder_output):\n",
    "        '''\n",
    "            hidden_state:decoder的隐层状态\n",
    "            encoder_output：encoder的输出\n",
    "        '''\n",
    "        # hidden_state shape == (batch_size, channel_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, channel_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
    "\n",
    "        # tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)) shape == \n",
    "        #                                            (batch_size, 7*7, channel_size)\n",
    "        # score shape == (batch_size, num_timesteps, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_output) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 7*7, 1)\n",
    "        # 对于当前步，特征图中(7*7*512中哪个位置比较重要, 即1*1*512\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    \n",
    "        # encoder_output shape ==     (batch_size, 7*7, channel_size)\n",
    "        # attention_weights shape == (batch_size, 7*7, 1)\n",
    "        # context_vector shape ==   (batch_size, channel_size)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention = BahdanauAttention()\n",
    "encoder_hidden = tf.random.normal([32, 512])\n",
    "attention_result, attention_weights = attention(encoder_hidden, encoder_output)\n",
    "print(\"attention_result shape is :\", attention_result.shape)\n",
    "print(\"attention_weights shape is :\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, channel_size, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        '''\n",
    "            vocab_size:词表的大小，用来生成词的概率分布，看哪个词的下标概率比较大\n",
    "            embedding_dim: 词embedding的维度\n",
    "            decoding_units： 循环神经网络的单元个数\n",
    "            channel_size：我们图片的channel数量\n",
    "            batch_size：批次的大小，因为我们数据是批次进来的\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.channel_size = channel_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding_layer = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        self.fc_context = keras.layers.Dense(embedding_dim)\n",
    "        self.fc_logits = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "        # 初始化 attention\n",
    "        self.attention = BahdanauAttention()\n",
    "\n",
    "    def call(self, x, hidden, encoding_output):\n",
    "        '''\n",
    "            encoding_output: 上一步的输出\n",
    "            hidden         : 上一步的状态\n",
    "            x              : 当前步的输入\n",
    "        '''\n",
    "        # 第一次hidden shape == (batch_size, channel_size)\n",
    "        # 第一次encoder_output shape == (batch_size, 7*7, channel_size)\n",
    "        \n",
    "        # context_vector  shape == (batch_size, channel_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_output)\n",
    "        \n",
    "        # x shape == (batch_size, embedding_dim )\n",
    "        x = self.embedding_layer(x)\n",
    "        # context_vector shape == (batch_size, channel_size) 维度和 x 维度是不匹配的，\n",
    "        # 所以加上一个全连接层，把它变成(batch_size, embedding_dim)\n",
    "        # context_vector shape == (batch_size, embedding_dim)\n",
    "        context_vector = self.fc_context(context_vector)\n",
    "        # 这一步就是把context_vector 和 当前步的输入 x 一起输入循环神经网络\n",
    "        # x shape == (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), tf.expand_dims(x, 1)], axis=-1)\n",
    "\n",
    "        # 经过 GRU 网络 # output shape == (batch_size, 1, decoding_units)\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size*1, decoding_units)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab_size)\n",
    "        # 也就是每个词的概率是多少，那个词概率大就取哪个值为预测的词\n",
    "        x = self.fc_logits(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "    # 对于每一个批次数据，第一次的时候，我们需要初始化我们的hidden_state，\n",
    "    # 因为我们图片没有经过循环神经网络，所以我们自己初始化\n",
    "    # 我们的Hidden_state要和encoder_output做attention，输入到循环神经网络中\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.channel_size))\n",
    "    \n",
    "decoder = Decoder(vocab_size, 64, 1024, 512, 32)\n",
    "decoder_output, _, _ = decoder(np.ones((32, 1)).reshape(-1,), encoder_hidden, encoder_output)\n",
    "print ('Decoder output shape: ', decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义学习率\n",
    "class CustomizedSchedule(\n",
    "    keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomizedSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "# 定义损失函数,因为我们的输出是经过激活函数的,所以from_logits=False\n",
    "# reduction='none' 表示我们要自己求和,因为有权重\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "batch_size = 100\n",
    "embedding_dim = 64\n",
    "decoding_units, channel_size = 512, 512\n",
    "num_timesteps = 20\n",
    "\n",
    "\n",
    "# 初始化我们的网络\n",
    "encoder = Encoder()\n",
    "decoder = Decoder(vocab.size(), embedding_dim, decoding_units, channel_size, batch_size)\n",
    "# 定义自适应学习率\n",
    "learning_rate = CustomizedSchedule(128)\n",
    "# 定义优化器\n",
    "optimizer = keras.optimizers.Adam(learning_rate,\n",
    "                                  beta_1=0.9,\n",
    "                                  beta_2=0.98,\n",
    "                                  epsilon=1e-9)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch_img_features, batch_sentence_ids, batch_weights, encoder_hidden):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # batch_img_features == (batch_size, 7, 7, channel_size)\n",
    "        # batch_sentence_ids == (batch_size, num_timesteps + 2) 因为有开始字符和结束字符\n",
    "        # batch_weights == (batch_size, num_timesteps + 2) 因为有开始字符和结束字符\n",
    "        \n",
    "        # encoding_hidden shape == [batch_size, channel_size]\n",
    "        encoding_output = encoder(batch_img_features)            \n",
    "        decoding_hidden = encoding_hidden\n",
    "        # decoder循环神经网络的工作流程\n",
    "        #    <s>      => 第一个词\n",
    "        # 第一个词 + context_vector     => 生成第二个词\n",
    "        # 第二个词     => 生成第三个词\n",
    "        #  .......   => .   .就是我们的结束字符\n",
    "        for t in range(num_timesteps + 2 - 1):\n",
    "            # decoding_input shape == (batch_size, 1) \n",
    "            decoding_input = batch_sentence_ids[:, t]\n",
    "            # predictions shape == (batch_size, vocab_size) \n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
    "            # labels hsape == (batch_size, 1)\n",
    "            # labels_weight shape == (batch_size, 1)\n",
    "            # 这边想一想为什么是 t+1 :<s> i love you . -> <s> i love -> i love you .\n",
    "            labels = batch_sentence_ids[:, t+1]\n",
    "            labels_weight = batch_weights[:, t+1]\n",
    "            loss_ = loss_object(labels, predictions)\n",
    "            labels_weight = tf.cast(labels_weight, loss_.dtype)\n",
    "            loss_ *= labels_weight\n",
    "            # 求和平均一下\n",
    "            loss_ = tf.reduce_mean(loss_)\n",
    "            loss += loss_\n",
    "            ######## 计算准确度，我们使用简单的方法，就是看对应位置，单词预测正确\n",
    "            pred_word_id = tf.argmax(predictions, 1, output_type = tf.int32)\n",
    "            correct_pred = tf.equal(pred_word_id, labels)\n",
    "            correct_prediction_with_mask = tf.multiply(tf.cast(correct_pred, tf.float32), \n",
    "                                                       labels_weight)\n",
    "            acc_ = tf.reduce_sum(correct_prediction_with_mask)\n",
    "            acc += acc_\n",
    "        batch_loss = (loss) / (batch_sentence_ids.shape[1])\n",
    "        batch_acc = (acc) / (batch_sentence_ids.shape[1])\n",
    "\n",
    "    trainable_variables =encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return batch_loss, batch_acc\n",
    "# 循环10次\n",
    "epoch_size = 50\n",
    "for epoch in range(epoch_size):\n",
    "    start = time.time() \n",
    "    # 生成我们的批训练数据集, 我们规定句子的长度为 num_timesteps\n",
    "    imgcaption_data = ImageCaptionData(img_name_to_token_ids, input_img_feature_dir, num_timesteps, vocab)\n",
    "\n",
    "    # 初始化hidden_state\n",
    "    encoding_hidden = decoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    # 看我们整个数据集，能跑几个batch\n",
    "    batchs_per_epoch = imgcaption_data.size() // batch_size\n",
    "    for batch in range(batchs_per_epoch):\n",
    "        # 名字我们就不要拿了\n",
    "        batch_img_features, batch_sentence_ids, batch_weights, _ = \\\n",
    "                                                            imgcaption_data.next(batch_size)\n",
    "        # 喂进去神经网络\n",
    "        batch_loss, batch_acc = train_step(batch_img_features, batch_sentence_ids, \n",
    "                                           batch_weights, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "        # 打印我们所关心的值\n",
    "#         print('Batch [{}]  Loss {:.10f} Acc {:.10f}'.format(batch, batch_loss.numpy(), batch_acc.numpy()))\n",
    "\n",
    "    print('Epoch [{}/{}]  Loss {:.10f}, Acc {:.10f}'.format(epoch + 1, epoch_size, \n",
    "                                                            total_loss.numpy()/batchs_per_epoch , \n",
    "                                                            total_acc.numpy()/batchs_per_epoch))\n",
    "    print('Time take for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gfile.GFile('../dataset/feature_extraction_inception_v3/image_features-0.pickle', 'rb') as f:\n",
    "    # 之前使用pickle存储的，现在我们使用pickle拿出来\n",
    "    filenames, features = pickle.load(f, encoding='iso-8859-1')\n",
    "test_feature = features[1]\n",
    "print(filenames[1])\n",
    "print(test_feature.shape)\n",
    "# 我们还要在 batch_szie 上扩充一个维度\n",
    "test_feature = np.expand_dims(test_feature, 0)\n",
    "print(test_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推理的时候就是\n",
    "# <s> => 第一个词\n",
    "# 第一个词     => 生成第二个词\n",
    "# 第二个词     => 生成第三个词\n",
    "#    ...       => .\n",
    "\n",
    "# 因为我们在 decoder hidden_state shape == [batch_size, channel_size]\n",
    "# encoding_hidden = decoder.initialize_hidden_state()\n",
    "# 但是在这里我们 batch_size = 1, 所以我们自己写一个就好\n",
    "hidden = tf.zeros((1, channel_size))\n",
    "encoding_out = encoder(test_feature)\n",
    "\n",
    "decoding_hidden = hidden\n",
    "# 开始字符作为我们的第一个输入\n",
    "decoding_input = np.array([vocab.start]).reshape(-1,)\n",
    "\n",
    "result = []\n",
    "for t in range(encoding_out.shape[1]):\n",
    "    predictions, decoding_hidden, attention_weights = decoder(\n",
    "        decoding_input, decoding_hidden, encoding_out)\n",
    "    predict_idx = tf.argmax(predictions[0]).numpy()\n",
    "    # 已经到了结束字符,那么我们就不应在预测了\n",
    "    if predict_idx == vocab.eos:\n",
    "        break\n",
    "    \n",
    "    result.append(predict_idx)\n",
    "    # 作为下一步的输入\n",
    "    decoding_input  = predict_idx.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"预测的id: \", result)\n",
    "print(\"真实的id\", img_name_to_token_ids['10002456.jpg'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"预测的结果:\", vocab.decode(result))\n",
    "print(\"真实的结果：\", vocab.decode(img_name_to_token_ids['10002456.jpg'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
