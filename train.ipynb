{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import gfile\n",
    "import tensorflow.compat.v1.logging as logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from tensorflow import keras\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # -1表示使用 cpu 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 设置内存自增长\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# # 打印物理GPU有几个，就是电脑实际装的个数\n",
    "# print(len(gpus))\n",
    "# for gpu in gpus:\n",
    "#     # 设置 GPU 所占用内存自动增长\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch_file = \"../dataset/data/train.ch\"   # 中文训练数据集\n",
    "input_en_file = \"../dataset/data/train.en\"   # 英文文训练数据集\n",
    "input_vocab_ch_file = \"../dataset/vocab_ch.txt\"\n",
    "input_vocab_en_file = \"../dataset/vocab_en.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "    <img src='../image/vocab_ch.png', width= 600, height = 200>\n",
    "    <img src='../image/vocab_en.png', width= 600, height = 200>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        '''\n",
    "            filename:文件的路径\n",
    "            word_num_threshold:如果单词出现的次数太少了,我们用 unk 进行代替\n",
    "        '''\n",
    "        # 用于解码， id到词的映射\n",
    "        self._id_to_word = {}\n",
    "        # 用于编码， 词到id的映射\n",
    "        self._word_to_id = {}\n",
    "        # 未知字符\n",
    "        self._unk = -1\n",
    "        # 开始字符\n",
    "        self._start = -1\n",
    "        # 结束字符\n",
    "        self._eos = -1\n",
    "        \n",
    "        # 前面我们统计过的词频，太少我们就不要了\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            # 词出现的次数\n",
    "            occurence = int(occurence)\n",
    "            # 如果词出现的频次太少了，我们就跳过\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            # 从顺序开始，第一次出现的词，对应的id为0， 以此类推\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word =='<s>':\n",
    "                self._start = idx\n",
    "            elif word == '</s>':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception(\"重复添加！！！\")\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    @property\n",
    "    def start(self):\n",
    "        return self._start\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "    # 单个词到id的转换\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "    # 单个id到词的转换\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')\n",
    "    # 整个词表的大小\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    # 编码，把句子转换成id  用于训练模型前的编码\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "    # 解码，把id数组转化成句子， 预测时，模型输出的是id， 用于解码\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把描述的句子转换成id\n",
    "def convert_token_to_id(filename, vocab):\n",
    "    '''\n",
    "        filename:输入的文件名字\n",
    "        vocab:词表,是上面实现的类\n",
    "    '''\n",
    "    word_to_token_ids = []\n",
    "    with gfile.GFile(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # 进行编码\n",
    "        token_ids = vocab.encode(line)\n",
    "        word_to_token_ids.append(token_ids)\n",
    "    return word_to_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_ch_size: 18807\n",
      "INFO:tensorflow:vocab_en_size: 14252\n",
      "INFO:tensorflow:num of all ch: 100000\n",
      "INFO:tensorflow:num of all en: 100000\n",
      "INFO:tensorflow:ch\n",
      "[1614,\n",
      " 3,\n",
      " 594,\n",
      " 122,\n",
      " 436,\n",
      " 3,\n",
      " 6,\n",
      " 3,\n",
      " 54,\n",
      " 87,\n",
      " 1140,\n",
      " 1134,\n",
      " 278,\n",
      " 5390,\n",
      " 44,\n",
      " 6149,\n",
      " 967,\n",
      " 4,\n",
      " 695,\n",
      " 3,\n",
      " 430,\n",
      " 1767,\n",
      " 3,\n",
      " 3201,\n",
      " 6,\n",
      " 3,\n",
      " 14171,\n",
      " 15368,\n",
      " 3,\n",
      " 83,\n",
      " 1552,\n",
      " 245,\n",
      " 48,\n",
      " 6423,\n",
      " 6149,\n",
      " 0]\n",
      "INFO:tensorflow:en\n",
      "[9,\n",
      " 1042,\n",
      " 4,\n",
      " 3,\n",
      " 0,\n",
      " 945,\n",
      " 3533,\n",
      " 190,\n",
      " 90,\n",
      " 5,\n",
      " 3,\n",
      " 0,\n",
      " 3533,\n",
      " 190,\n",
      " 90,\n",
      " 93,\n",
      " 1502,\n",
      " 1342,\n",
      " 5,\n",
      " 1065,\n",
      " 17,\n",
      " 11,\n",
      " 1571,\n",
      " 363,\n",
      " 449,\n",
      " 1131,\n",
      " 3815,\n",
      " 948,\n",
      " 108,\n",
      " 3,\n",
      " 1207,\n",
      " 4,\n",
      " 2707,\n",
      " 82,\n",
      " 20,\n",
      " 4,\n",
      " 5,\n",
      " 463,\n",
      " 8,\n",
      " 340,\n",
      " 54,\n",
      " 654,\n",
      " 12,\n",
      " 8,\n",
      " 12,\n",
      " 654,\n",
      " 1131,\n",
      " 290,\n",
      " 3,\n",
      " 29,\n",
      " 6,\n",
      " 47,\n",
      " 1243,\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "vocab_ch = Vocab(input_vocab_ch_file, 5)\n",
    "vocab_en = Vocab(input_vocab_en_file, 5)\n",
    "vocab_ch_size = vocab_ch.size()\n",
    "vocab_en_size = vocab_en.size()\n",
    "logging.info(\"vocab_ch_size: %d\" % vocab_ch_size)\n",
    "logging.info(\"vocab_en_size: %d\" % vocab_en_size)\n",
    "\n",
    "word_ch_to_token_ids = convert_token_to_id(input_ch_file, vocab_ch)\n",
    "word_en_to_token_ids = convert_token_to_id(input_en_file, vocab_en)\n",
    "\n",
    "\n",
    "logging.info(\"num of all ch: %d\" % len(word_ch_to_token_ids))\n",
    "logging.info(\"num of all en: %d\" % len(word_en_to_token_ids))\n",
    "\n",
    "logging.info(\"ch\")\n",
    "pprint.pprint(word_ch_to_token_ids[0])\n",
    "logging.info(\"en\")\n",
    "pprint.pprint(word_en_to_token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "    <img src='../image/train_ch.png', width= 600, height = 200>\n",
    "    <img src='../image/vocab_ch.png', width= 600, height = 200>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "产生批量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:translate_data_size: 100000\n",
      "INFO:tensorflow:中文取一个批次的训练数据集\n",
      "array([[    1,  1614,     3,   594,   122,   436,     3,     6,     3,\n",
      "           54,    87,  1140,  1134,   278,  5390,    44,  6149,   967,\n",
      "            4,   695,     3,     2],\n",
      "       [    1,     9,   843,  5576,     3,   245,     9, 11060,     3,\n",
      "            0,     3,  6046,     6,    88,     6,     3,   346,     6,\n",
      "            3,  9036,  1474,     2],\n",
      "       [    1,     6,     6,     6,    19,   847,  5478,  2830,  2572,\n",
      "          511,  6149,   967,   564,     4,  2133,     3,    97,    89,\n",
      "         6285,   239,    25,     2],\n",
      "       [    1,     6,  1519,    32,  9741,  3873, 11061,  9742,  1780,\n",
      "          333,     6,  4059,    23,    42,     7,    33,    28,     8,\n",
      "           33,  1186,  1231,     2],\n",
      "       [    1,   278,  2869,    94,     3,   116,   751,  3201,  2036,\n",
      "            3,  1642,   751,   509,    32,  4579, 16880,    49,  1015,\n",
      "         4452,     0,     2,     2]])\n",
      "array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "INFO:tensorflow:英文文取一个批次的训练数据集\n",
      "array([[    1,     9,  1042,     4,     3,     0,   945,  3533,   190,\n",
      "           90,     5,     3,     0,  3533,   190,    90,    93,  1502,\n",
      "         1342,     5,  1065,     2],\n",
      "       [    1,     9,   149,   383,   226,     4,     3,   285,  7477,\n",
      "           17,     3,  6490,     4,     9,     3, 12138,   463,     4,\n",
      "           76,   138,     3,     2],\n",
      "       [    1,    36,   524,  1302,    38,     0,  1515,     6,     0,\n",
      "         1650,     9,  8121,   887,   819,   177,    11,  1131,  1486,\n",
      "          320,    25,   771,     2],\n",
      "       [    1,    20,     4,    11,  9353,    38,    20,  1515,     6,\n",
      "           20,  1650,     9,  8121,   328,     4,    45,    44,    10,\n",
      "           11,    78,    13,     2],\n",
      "       [    1,   124,   195,   102,   906,   486,     4,   195,  1134,\n",
      "           14,    27,   146,    28,   680,    16,    42,     8,  4228,\n",
      "            3,  2707,   626,     2]])\n",
      "array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "class TranslateData(object):\n",
    "    def __init__(self,\n",
    "                 word_ch_to_token_ids,\n",
    "                 word_en_to_token_ids,\n",
    "                 num_timesteps,\n",
    "                 vocab_ch,\n",
    "                 vocab_en,\n",
    "                 deterministic = False):\n",
    "        '''\n",
    "            word_ch_to_token_ids:\n",
    "            word_en_to_token_ids:  句子到id的映射\n",
    "            num_timesteps:固定句子的长度,因为我们知道有些句子很长,但是出现的次数是很少的\n",
    "            vocab_ch:\n",
    "            vocab_en: 词表\n",
    "            deterministic:是否进行shuffle, 默认是进行shuffle\n",
    "        '''\n",
    "        self._vocab_ch = vocab_ch  \n",
    "        self._vocab_en = vocab_en\n",
    "    \n",
    "        self._word_ch_to_token_ids = word_ch_to_token_ids\n",
    "        self._word_en_to_token_ids = word_en_to_token_ids\n",
    "        # 固定一个句子的长度\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # 其时批次的下标\n",
    "        self._indicator = 0\n",
    "        # 是否进行shuffle\n",
    "        self._deterministic = deterministic\n",
    "    # 有多少个训练样本,英文和中文的长度是一样\n",
    "    def size(self):\n",
    "        assert len(self._word_ch_to_token_ids) == len(self._word_en_to_token_ids)\n",
    "        return len(self._word_ch_to_token_ids)\n",
    "    # 进行下标打乱\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._word_ch_to_token_ids = self._word_ch_to_token_ids[p]\n",
    "        self._word_en_to_token_ids = self._word_en_to_token_ids[p]\n",
    "    # 转成id\n",
    "    def _sentence_ids(self, word_to_token_ids, vocab):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for i in range(len(word_to_token_ids)):\n",
    "            chosen_token_ids = word_to_token_ids[i]\n",
    "            # 拿到当前句子的长度\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "            # 为什么会有weight， 因为描述的长度小于我们固定的长度，那么我们不做长句的惩罚.\n",
    "            # 假设我们选取的句子长度为5\n",
    "            # 我 爱 你 . ..  真实: i love you 模型：i love you very much.   weight:[1, 1, 1, 0, 0]\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            # 如果句子长度大于我们固定的长度\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            # 对于句子,我们需要在最开始加上开始符号,最后面加上符号\n",
    "            chosen_token_ids.insert(0, 1)\n",
    "            chosen_token_ids.append(2)\n",
    "            # 对于这两个地方,我们就不计算权重\n",
    "            weight.insert(0, 0)\n",
    "            weight.append(0)\n",
    "            \n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "    # 下一个批次\n",
    "    def next(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        # 如果已经取到文件末尾了，是否进行shuffle，还有就是把其实的index置为0\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "        # 采用切片,进行一个批次数据的截取\n",
    "        batch_word_ch_token_ids = self._word_ch_to_token_ids[self._indicator: end_indicator]\n",
    "        batch_word_en_token_ids = self._word_en_to_token_ids[self._indicator: end_indicator]\n",
    "        # 产生对应长度的句子和权重\n",
    "        batch_ch_sentence_ids, batch_ch_weights = self._sentence_ids(batch_word_ch_token_ids, self._vocab_ch)\n",
    "        batch_en_sentence_ids, batch_en_weights = self._sentence_ids(batch_word_en_token_ids, self._vocab_en)\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_ch_sentence_ids, batch_ch_weights, batch_en_sentence_ids, batch_en_weights\n",
    "\n",
    "\n",
    "translate_data = TranslateData(word_ch_to_token_ids, word_en_to_token_ids, \\\n",
    "                                20, vocab_ch, vocab_en)\n",
    "translate_data_size = translate_data.size()\n",
    "logging.info(\"translate_data_size: %d\" % translate_data_size)\n",
    "\n",
    "batch_ch_sentence_ids, batch_ch_weights, batch_en_sentence_ids, batch_en_weights = translate_data.next(5)\n",
    "logging.info(\"中文取一个批次的训练数据集\")\n",
    "pprint.pprint(batch_ch_sentence_ids)\n",
    "pprint.pprint(batch_ch_weights)\n",
    "logging.info(\"英文文取一个批次的训练数据集\")\n",
    "pprint.pprint(batch_en_sentence_ids)\n",
    "pprint.pprint(batch_en_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder网络的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  align=\"center\">    \n",
    "    <img src='../image/框架图.png', width= 600, height = 200>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output size is : (5, 22, 64)\n",
      "encoder_state size is : (5, 64)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        '''\n",
    "            vocab_size:词表的个数\n",
    "            embedding_dim: 单词的embedding的维度\n",
    "            encoding_units:循环神经网络的单元个数\n",
    "            batch_size:每个批次数据的大小\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding_layer = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    @tf.function\n",
    "    def call(self, x, hidden):\n",
    "        # x shape == (batch_size, num_timesteps+2), 句子的长度 + 2 (开始和结束字符)\n",
    "        # after embedding layer x shape == (batch, num_timesteps+2, embedding_dim)\n",
    "        x = self.embedding_layer(x)\n",
    "        # output shape == (batch_size, num_timesteps+2, encoding_units)\n",
    "        # state shape == (batch_size, encoding_units)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    # 为什么要有这个初始化的hidden呢, 因为后面进行预测的时候,我们需要一样的hidden, 所以全为零,才一致\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "inputs = batch_ch_sentence_ids\n",
    "encoder = Encoder(vocab_ch.size(), 64, 64, 5)\n",
    "encoder_hidden = encoder.initialize_hidden_state()\n",
    "encoder_output, encoder_state = encoder(inputs, encoder_hidden)\n",
    "print(\"encoder_output size is :\", encoder_output.shape)\n",
    "print(\"encoder_state size is :\", encoder_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_result shape is : (5, 64)\n",
      "attention_weights shape is : (5, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, hidden_state, encoder_output):\n",
    "        '''\n",
    "            hidden_state:隐层 \n",
    "            encoder_output: encoder 的输出\n",
    "            \n",
    "        '''\n",
    "        # hidden shape == (batch_size, encoding_units)\n",
    "        # encoder_output == (batch_size, num_timesteps+2, encoding_units)\n",
    "        \n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
    "\n",
    "        # score shape == (batch_size, num_timesteps+2, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size,  num_timesteps+2, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_output) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, num_timesteps+2, 1)\n",
    "        # 这边就是求词的权重,所以是第二个维度\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # values shape ==           (batch_size, num_timesteps+2, encoding_units)\n",
    "        # attention_weights shape == (batch_size, num_timesteps+2, 1)\n",
    "        # context_vector shape ==   (batch_size, encoding_units)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention = BahdanauAttention(64)\n",
    "attention_result, attention_weights = attention(encoder_hidden, encoder_output)\n",
    "print(\"attention_result shape is :\", attention_result.shape)\n",
    "print(\"attention_weights shape is :\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 20), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.ones((5, 1))\n",
    "v = tf.ones((5, 20))\n",
    "w * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape:  (5, 14252)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, batch_size):\n",
    "        '''\n",
    "            vocab_size:英文词表的大小(因为 decoder 本来就是对英文做的)\n",
    "            embedding_dim: 词embedding的维度\n",
    "            decoding_units: decoder的维度\n",
    "            batch_size:批次数据集的大小\n",
    "        '''\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding_layer = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size, activation='softmax')\n",
    "\n",
    "        # 初始化 attention\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "\n",
    "    def call(self, x, hidden_state, encoding_output):\n",
    "        '''\n",
    "            encoding_output: 上一步的输出\n",
    "            hidden_state   : 上一步的状态\n",
    "            x              : 当前步的输入\n",
    "        '''\n",
    "        # enc_output shape == (batch_size, num_timesteps+2, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden_state, encoding_output)\n",
    "        # x shape == (batch_size, 1), \n",
    "        # after embedding layer x shape == (batch, embedding_dim)\n",
    "        x = self.embedding_layer(x)\n",
    "        x = tf.expand_dims(x, 1)\n",
    "        # x shape == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # 经过 GRU 网络 # output shape == (batch_size, 1, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size*1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab_size)\n",
    "        # 也就是每个词的概率是多少\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(vocab_en.size(), 64, 64, 5)\n",
    "decoder_output, _, _ = decoder(batch_en_sentence_ids[:, 0],\n",
    "                               encoder_hidden, \n",
    "                               encoder_output)\n",
    "print ('Decoder output shape: ', decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]  Loss 6.3368046875, Acc 6.0101333008\n",
      "Time take for 1 epoch: 607.8969831466675 secs\n",
      "\n",
      "Epoch [2/50]  Loss 5.4098110352, Acc 13.3182216797\n",
      "Time take for 1 epoch: 645.712822675705 secs\n",
      "\n",
      "Epoch [3/50]  Loss 4.7380170898, Acc 20.4872031250\n",
      "Time take for 1 epoch: 614.4036946296692 secs\n",
      "\n",
      "Epoch [4/50]  Loss 4.3464741211, Acc 23.7130468750\n",
      "Time take for 1 epoch: 666.1944766044617 secs\n",
      "\n",
      "Epoch [5/50]  Loss 4.0523605957, Acc 26.1925039062\n",
      "Time take for 1 epoch: 796.5499594211578 secs\n",
      "\n",
      "Epoch [6/50]  Loss 3.8465437012, Acc 27.9809179687\n",
      "Time take for 1 epoch: 647.4665360450745 secs\n",
      "\n",
      "Epoch [7/50]  Loss 3.6935634766, Acc 29.3933847656\n",
      "Time take for 1 epoch: 625.9186623096466 secs\n",
      "\n",
      "Epoch [8/50]  Loss 3.5671892090, Acc 30.6027304687\n",
      "Time take for 1 epoch: 612.6435477733612 secs\n",
      "\n",
      "Epoch [9/50]  Loss 3.4627448730, Acc 31.6496718750\n",
      "Time take for 1 epoch: 600.4327054023743 secs\n",
      "\n",
      "Epoch [10/50]  Loss 3.3737534180, Acc 32.5703320312\n",
      "Time take for 1 epoch: 631.5030176639557 secs\n",
      "\n",
      "Epoch [11/50]  Loss 3.2953552246, Acc 33.3950234375\n",
      "Time take for 1 epoch: 617.1295671463013 secs\n",
      "\n",
      "Epoch [12/50]  Loss 3.2271589355, Acc 34.1226992188\n",
      "Time take for 1 epoch: 620.8087587356567 secs\n",
      "\n",
      "Epoch [13/50]  Loss 3.1666328125, Acc 34.7569687500\n",
      "Time take for 1 epoch: 653.732396364212 secs\n",
      "\n",
      "Epoch [14/50]  Loss 3.1098776855, Acc 35.3883476562\n",
      "Time take for 1 epoch: 626.605783700943 secs\n",
      "\n",
      "Epoch [15/50]  Loss 3.0579094238, Acc 35.9659531250\n",
      "Time take for 1 epoch: 601.5462172031403 secs\n",
      "\n",
      "Epoch [16/50]  Loss 3.0112268066, Acc 36.4709765625\n",
      "Time take for 1 epoch: 612.7477848529816 secs\n",
      "\n",
      "Epoch [17/50]  Loss 2.9695544434, Acc 36.9308515625\n",
      "Time take for 1 epoch: 598.8264875411987 secs\n",
      "\n",
      "Epoch [18/50]  Loss 2.9281999512, Acc 37.4262109375\n",
      "Time take for 1 epoch: 728.273833990097 secs\n",
      "\n",
      "Epoch [19/50]  Loss 2.8894296875, Acc 37.8626679688\n",
      "Time take for 1 epoch: 724.5746099948883 secs\n",
      "\n",
      "Epoch [20/50]  Loss 2.8502583008, Acc 38.3485703125\n",
      "Time take for 1 epoch: 600.0877923965454 secs\n",
      "\n",
      "Epoch [21/50]  Loss 2.8186103516, Acc 38.7252304687\n",
      "Time take for 1 epoch: 636.7293872833252 secs\n",
      "\n",
      "Epoch [22/50]  Loss 2.7904560547, Acc 39.0659609375\n",
      "Time take for 1 epoch: 697.3303663730621 secs\n",
      "\n",
      "Epoch [23/50]  Loss 2.7627448730, Acc 39.3915468750\n",
      "Time take for 1 epoch: 732.1813225746155 secs\n",
      "\n",
      "Epoch [24/50]  Loss 2.7310185547, Acc 39.7933046875\n",
      "Time take for 1 epoch: 627.9227204322815 secs\n",
      "\n",
      "Epoch [25/50]  Loss 2.7074294434, Acc 40.0894687500\n",
      "Time take for 1 epoch: 628.4153928756714 secs\n",
      "\n",
      "Epoch [26/50]  Loss 2.6872211914, Acc 40.3277695312\n",
      "Time take for 1 epoch: 629.9512979984283 secs\n",
      "\n",
      "Epoch [27/50]  Loss 2.6643806152, Acc 40.6157578125\n",
      "Time take for 1 epoch: 637.5344321727753 secs\n",
      "\n",
      "Epoch [28/50]  Loss 2.6448718262, Acc 40.8748242187\n",
      "Time take for 1 epoch: 620.5202958583832 secs\n",
      "\n",
      "Epoch [29/50]  Loss 2.6252397461, Acc 41.1424648437\n",
      "Time take for 1 epoch: 626.8735671043396 secs\n",
      "\n",
      "Epoch [30/50]  Loss 2.6013771973, Acc 41.4779179687\n",
      "Time take for 1 epoch: 631.2015914916992 secs\n",
      "\n",
      "Epoch [31/50]  Loss 2.5867919922, Acc 41.6539531250\n",
      "Time take for 1 epoch: 592.9157183170319 secs\n",
      "\n",
      "Epoch [32/50]  Loss 2.5664653320, Acc 41.9134960937\n",
      "Time take for 1 epoch: 577.8944320678711 secs\n",
      "\n",
      "Epoch [33/50]  Loss 2.5422780762, Acc 42.2522148437\n",
      "Time take for 1 epoch: 573.0620889663696 secs\n",
      "\n",
      "Epoch [34/50]  Loss 2.5309448242, Acc 42.3938867187\n",
      "Time take for 1 epoch: 589.4442601203918 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_22656/1667931658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# 喂进去神经网络\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         batch_loss, batch_acc = train_step(batch_ch_sentence_ids, \n\u001b[0m\u001b[0;32m    108\u001b[0m                                            \u001b[0mbatch_ch_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                                            \u001b[0mbatch_en_sentence_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 自定义学习率\n",
    "class CustomizedSchedule(\n",
    "    keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomizedSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "# 定义损失函数,因为我们的输出是经过激活函数的,所以from_logits=False\n",
    "# reduction='none' 表示我们要自己求和,因为有权重\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "batch_size = 100\n",
    "embedding_dim = 64\n",
    "encoding_units, decoding_units = 128, 128\n",
    "num_timesteps = 20\n",
    "\n",
    "\n",
    "# 初始化我们的网络\n",
    "# encoder对中文编码，所以要输入中文的词表大小\n",
    "encoder = Encoder(vocab_ch.size(), embedding_dim, encoding_units, batch_size)\n",
    "# decocder是针对英文的\n",
    "decoder = Decoder(vocab_en.size(), embedding_dim, decoding_units, batch_size)\n",
    "# 定义自适应学习率\n",
    "learning_rate = CustomizedSchedule(128)\n",
    "# 定义优化器\n",
    "optimizer = keras.optimizers.Adam(learning_rate,\n",
    "                                  beta_1=0.9,\n",
    "                                  beta_2=0.98,\n",
    "                                  epsilon=1e-9)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch_ch_sentence_ids, batch_ch_weights, batch_en_sentence_ids, batch_en_weights, encoder_hidden):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # batch_sentence_ids == (batch_size, num_timesteps + 2) 因为有开始字符和结束字符\n",
    "        # batch_weights == (batch_size, num_timesteps + 2) 因为有开始字符和结束字符\n",
    "        \n",
    "        # encoding_hidden shape == [batch_size, encoder_units]\n",
    "        encoding_output, encoding_hidden = encoder(batch_ch_sentence_ids, encoder_hidden)            \n",
    "        decoding_hidden = encoding_hidden\n",
    "        # decoder循环神经网络的工作流程\n",
    "        #    <s> +  encoding_hidden    => 第一个词\n",
    "        # 第一个词 + context_vector    => 生成第二个词\n",
    "        # 第二个词     => 生成第三个词\n",
    "        #  .......   => </s>\n",
    "        for t in range(num_timesteps + 2 - 1):\n",
    "            # decoding_input shape == (batch_size, 1) \n",
    "            decoding_input = batch_en_sentence_ids[:, t]\n",
    "            # predictions shape == (batch_size, vocab_size) \n",
    "            predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
    "            # labels hsape == (batch_size, 1)\n",
    "            # labels_weight shape == (batch_size, 1)\n",
    "            # 这边想一想为什么是 t+1 :<s> i love you . -> <s> i love -> i love you .\n",
    "            # 解码这边是对英文做的，所以拿的都是 en\n",
    "            labels = batch_en_sentence_ids[:, t+1]\n",
    "            labels_weight = batch_en_weights[:, t+1]\n",
    "            loss_ = loss_object(labels, predictions)\n",
    "            labels_weight = tf.cast(labels_weight, loss_.dtype)\n",
    "            loss_ *= labels_weight\n",
    "            # 求和平均一下\n",
    "            loss_ = tf.reduce_mean(loss_)\n",
    "            loss += loss_\n",
    "            ######## 计算准确度，我们使用简单的方法，就是看对应位置，单词预测正确\n",
    "            pred_word_id = tf.argmax(predictions, 1, output_type = tf.int32)\n",
    "            correct_pred = tf.equal(pred_word_id, labels)\n",
    "            correct_prediction_with_mask = tf.multiply(tf.cast(correct_pred, tf.float32), \n",
    "                                                       labels_weight)\n",
    "            acc_ = tf.reduce_sum(correct_prediction_with_mask)\n",
    "            acc += acc_\n",
    "        batch_loss = (loss) / (batch_en_sentence_ids.shape[1])\n",
    "        batch_acc = (acc) / (batch_en_sentence_ids.shape[1])\n",
    "\n",
    "    trainable_variables =encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(batch_loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return batch_loss, batch_acc\n",
    "# 循环10次\n",
    "epoch_size = 50\n",
    "for epoch in range(epoch_size):\n",
    "    start = time.time() \n",
    "    # 生成我们的批训练数据集, 我们规定句子的长度为 num_timesteps\n",
    "    translate_data = TranslateData(word_ch_to_token_ids, word_en_to_token_ids, \\\n",
    "                                num_timesteps, vocab_ch, vocab_en)\n",
    "\n",
    "    # 初始化hidden_state\n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    # 看我们整个数据集，能跑几个batch\n",
    "    batchs_per_epoch = translate_data.size() // batch_size\n",
    "    for batch in range(batchs_per_epoch):\n",
    "        batch_ch_sentence_ids, batch_ch_weights, batch_en_sentence_ids, batch_en_weights \\\n",
    "                                                            = translate_data.next(batch_size)\n",
    "\n",
    "        # 喂进去神经网络\n",
    "        batch_loss, batch_acc = train_step(batch_ch_sentence_ids, \n",
    "                                           batch_ch_weights, \n",
    "                                           batch_en_sentence_ids, \n",
    "                                           batch_en_weights, encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "        # 打印我们所关心的值\n",
    "#         print('Batch [{}]  Loss {:.10f} Acc {:.10f}'.format(batch, batch_loss.numpy(), batch_acc.numpy()))\n",
    "\n",
    "    print('Epoch [{}/{}]  Loss {:.10f}, Acc {:.10f}'.format(epoch + 1, epoch_size, \n",
    "                                                            total_loss.numpy()/batchs_per_epoch , \n",
    "                                                            total_acc.numpy()/batchs_per_epoch))\n",
    "    print('Time take for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对我们模型进行测试，为了方便，我们使用训练数据集中的数据就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 22, 128)\n",
      "预测的结果:  [124, 195, 4, 195, 610, 14, 3, 29, 6, 3, 29, 8, 3, 20, 8, 3, 20, 8, 3, 20]\n"
     ]
    }
   ],
   "source": [
    "# 推理的时候就是\n",
    "# <s>         => 第一个词\n",
    "# 第一个词     => 生成第二个词\n",
    "# 第二个词     => 生成第三个词\n",
    "#    ...      => </s>\n",
    "\n",
    "word_test_ch_to_token_ids_one = word_ch_to_token_ids[4]\n",
    "word_test_ch_to_token_ids_one = np.asarray(word_test_ch_to_token_ids_one)\n",
    "word_test_ch_to_token_ids_one = word_test_ch_to_token_ids_one.reshape((1, -1))\n",
    "\n",
    "# 因为我们在 encode hidden shape == [batch_size, units]\n",
    "hidden = [tf.zeros((1, encoding_units))]\n",
    "encoding_out, encoding_hidden = encoder(word_test_ch_to_token_ids_one, hidden)\n",
    "print(encoding_out.shape)\n",
    "\n",
    "decoding_hidden = encoding_hidden\n",
    "decoding_input = np.array([vocab_en.start])\n",
    "result = []\n",
    "for t in range(num_timesteps):\n",
    "    predictions, decoding_hidden, attention_weights = decoder(\n",
    "        decoding_input, decoding_hidden, encoding_out)\n",
    "    \n",
    "#     predict_idx = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "    predict_idx = tf.argmax(predictions[0]).numpy()\n",
    "    # 已经到了结束字符,那么我们就不应在预测了\n",
    "    if predict_idx == vocab_en.eos:\n",
    "        break\n",
    "    \n",
    "    result.append(predict_idx)\n",
    "    # 然后扩充1个维度\n",
    "    decoding_input  = tf.expand_dims(predict_idx , 0)\n",
    "\n",
    "print(\"预测的结果: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的id: [124, 195, 102, 906, 486, 4, 195, 1134, 14, 27, 146, 28, 680, 16, 42, 8, 4228, 3, 2707, 626]\n"
     ]
    }
   ],
   "source": [
    "print(\"原始的id:\", word_en_to_token_ids[4][:num_timesteps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的句子 when i , i believe that the people of the people to the <unk> to the <unk> to the <unk>\n",
      "原始的句子 when i first came here , i thought that it would be difficult for us to gather the villagers together\n"
     ]
    }
   ],
   "source": [
    "print(\"预测的句子\", vocab_en.decode(result))\n",
    "print(\"原始的句子\", vocab_en.decode(word_en_to_token_ids[4][:num_timesteps]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来测试一下比较简单的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 128)\n",
      "预测的结果:  [195, 2262, 789, 39, 29]\n"
     ]
    }
   ],
   "source": [
    "# 推理的时候就是\n",
    "# <s>         => 第一个词\n",
    "# 第一个词     => 生成第二个词\n",
    "# 第二个词     => 生成第三个词\n",
    "#    ...      => </s>\n",
    "\n",
    "test_sentence = \"我 爱 你 中国\"\n",
    "word_test_ch_to_token_ids_one = vocab_ch.encode(test_sentence)\n",
    "word_test_ch_to_token_ids_one = np.asarray(word_test_ch_to_token_ids_one)\n",
    "word_test_ch_to_token_ids_one = word_test_ch_to_token_ids_one.reshape((1, -1))\n",
    "\n",
    "# 因为我们在 encode hidden shape == [batch_size, units]\n",
    "hidden = [tf.zeros((1, encoding_units))]\n",
    "encoding_out, encoding_hidden = encoder(word_test_ch_to_token_ids_one, hidden)\n",
    "print(encoding_out.shape)\n",
    "\n",
    "decoding_hidden = encoding_hidden\n",
    "decoding_input = np.array([vocab_en.start])\n",
    "result = []\n",
    "for t in range(word_test_ch_to_token_ids_one.shape[1]):\n",
    "    predictions, decoding_hidden, attention_weights = decoder(\n",
    "        decoding_input, decoding_hidden, encoding_out)\n",
    "    \n",
    "    predict_idx = tf.argmax(predictions[0]).numpy()\n",
    "    # 已经到了结束字符,那么我们就不应在预测了\n",
    "    if predict_idx == vocab_en.eos:\n",
    "        break\n",
    "    \n",
    "    result.append(predict_idx)\n",
    "    # 然后扩充1个维度\n",
    "    decoding_input  = tf.expand_dims(predict_idx , 0)\n",
    "\n",
    "print(\"预测的结果: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的句子 i love your chinese people\n"
     ]
    }
   ],
   "source": [
    "print(\"预测的句子\", vocab_en.decode(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
